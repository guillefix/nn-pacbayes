#!/bin/bash

dataset=$1
net=$2
pool=$3
number_inits=$4
#index=$5
intermediate_pooling=0000
L=2
if [ $net = fc ]; then
    L=2
elif [ $net = cnn ]; then
    L=4
    if [ $pool = none ]; then
        intermediate_pooling=0000
    else
        intermediate_pooling=1111
    fi
fi
#optimizer=adam
optimizer=sgd
loss=ce
sigmaw=1.41
#sigmaw=10.0
sigmab=0.0
#sigmab=10.0
c=0.0
epochs_after_fit=0
#prefix=new_mother_of_all_msweeps_
prefix=grandmother_of_all_msweeps_
#prefix=test_new_mother_of_all_msweeps_
#prefix=analytical_2jade_new_msweep_
#prefix=big_b_3jade_new_msweep_
#prefix=2gpu_msweep_
#prefix=3jade_new_msweep_
#prefix=analytical_2jade_new_msweep_
#prefix=test_msweep_
#prefix=cifar_test_

export n_gpus=0
export n_procs=$number_inits

#for m in 1 3 11 36 122 407 1357 4516 15026 49999; do
#for m in 1 3 11 36 122 407 1357 4516 ; do
#for m in 4516; do
#for m in 10000; do
#for m in 15026; do
#for m in 15026 40000; do
for m in 40000; do
#for m in 49999; do
#for m in 1 ; do
echo $dataset $net $pool
echo $m
## msweep boolean 
  #./run_experiment_msweep --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --training --sigmaw $sigmaw --sigmab $sigmab --confusion $c --n_gpus $n_gpus --pooling $pool --number_inits $number_inits --n_samples_repeats 0.1 --epochs_after_fit $epochs_after_fit -loss $loss --optimizer $optimizer --use_empirical_K --intermediate_pooling $intermediate_pooling 
  #./run_experiment_msweep2 --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --training --sigmaw $sigmaw --sigmab $sigmab --confusion $c --n_gpus $n_gpus --pooling $pool --number_inits $number_inits --n_samples_repeats 0.1 --epochs_after_fit $epochs_after_fit -loss $loss --optimizer $optimizer --intermediate_pooling $intermediate_pooling --kernel_mult 1 --empirical_kernel_batch_size 128 --normalize_kernel --use_empirical_K  --batch_size 256 --learning_rate 0.01 --layer_widths 1024 --partial_kernel_index $index --partial_kernel_n_proc 89 --store_partial_kernel
  ./run_experiment_msweep23 --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --training --sigmaw $sigmaw --sigmab $sigmab --confusion $c --n_gpus $n_gpus --pooling $pool --number_inits $number_inits --n_samples_repeats 0.1 --epochs_after_fit $epochs_after_fit -loss $loss --optimizer $optimizer --intermediate_pooling $intermediate_pooling --kernel_mult 1 --empirical_kernel_batch_size 128 --normalize_kernel --use_empirical_K  --batch_size 128 --learning_rate 0.001 --layer_widths 1024
done
