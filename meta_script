#!/bin/bash

n_gpus=0
n_procs=1
train_method=NN
generate_arch=0
generate_data=0
compute_kernel=0
compute_bound=1
train=0
sample_funs=0
number_inits=10000000

export py=python3
export mpi=mpiexec
#py=/users/guillefix/anaconda3/envs/venv/bin/python
#mpi=/users/guillefix/anaconda3/envs/venv/bin/mpiexec
export n_gpus n_procs train_method generate_arch generate_data compute_kernel compute_bound train sample_funs py mpi

m=100
dataset=mnist
#boolfun=00001110110011111001111111001111000000000000000000000000000000000000000001001100000000001101110100000000000000000000000000000000
#boolfun_comp=84.0
#net=cnn
net=fc
L=2
optimizer=sgd
loss=ce
#loss=mse
sigmaw=1.41
#sigmaw=50.0
sigmab=0.0
pool=none
c=0.0
batch_size=32
epochs_after_fit=0
#epochs_after_fit=64
#prefix=oddeven_big_sgd_${loss}_sample_${loss}_
prefix=test_

if [ $loss = ce ]; then
    kern_mult=10000
else
    kern_mult=1
fi

#for m in 4 16 32 64 128 256 512 1024 2048 4096 8192; do
#echo $m
  ./run_experiment --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --sigmaw $sigmaw --sigmab $sigmab --n_gpus $n_gpus --pooling $pool -loss $loss --n_samples_repeats 0.1 --kernel_mult $kern_mult --optimizer $optimizer --norandom_training_set --number_inits $number_inits --binarization_method=oddeven --batch_size=$batch_size
#done
