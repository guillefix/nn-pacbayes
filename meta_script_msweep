#!/bin/bash

dataset=$1
net=$2
pool=$3
number_inits=$4
intermediate_pooling=0000
L=2
if [ $net = fc ]; then
    L=2
elif [ $net = cnn ]; then
    L=4
    if [ $pool = none ]; then
        intermediate_pooling=0000
    else
        intermediate_pooling=1111
    fi
fi
optimizer=adam
#optimizer=sgd
loss=ce
sigmaw=1.41
#sigmaw=10.0
sigmab=0.0
#sigmab=10.0
c=0.0
epochs_after_fit=0
prefix=new_mother_of_all_msweeps_
#prefix=analytical_2jade_new_msweep_
#prefix=big_b_3jade_new_msweep_
#prefix=2gpu_msweep_
#prefix=3jade_new_msweep_
#prefix=analytical_2jade_new_msweep_
#prefix=test_msweep_

export n_gpus=1
export n_procs=$number_inits

#for m in 1 3 11 36 122 407 1357 4516 15026 49999; do
#for m in 1 3 11 36 122 407 1357 4516 ; do
for m in 15026 40000; do
#for m in 49999; do
#for m in 1 ; do
echo $m
## msweep boolean 
  #./run_experiment_msweep --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --training --sigmaw $sigmaw --sigmab $sigmab --confusion $c --n_gpus $n_gpus --pooling $pool --number_inits $number_inits --n_samples_repeats 0.1 --epochs_after_fit $epochs_after_fit -loss $loss --optimizer $optimizer --use_empirical_K --intermediate_pooling $intermediate_pooling 
  ./run_experiment_msweep2 --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --training --sigmaw $sigmaw --sigmab $sigmab --confusion $c --n_gpus $n_gpus --pooling $pool --number_inits $number_inits --n_samples_repeats 0.1 --epochs_after_fit $epochs_after_fit -loss $loss --optimizer $optimizer --intermediate_pooling $intermediate_pooling --kernel_mult 1 --normalize_kernel --empirical_kernel_batch_size 1024 --layer_widths 1024 --learning_rate 0.01 --batch_size 256
  #./run_experiment_msweep2 --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --training --sigmaw $sigmaw --sigmab $sigmab --confusion $c --n_gpus $n_gpus --pooling $pool --number_inits $number_inits --n_samples_repeats 0.1 --epochs_after_fit $epochs_after_fit -loss $loss --optimizer $optimizer --use_empirical_K --intermediate_pooling $intermediate_pooling --kernel_mult 1 --empirical_kernel_batch_size 1024
done
