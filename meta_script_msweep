#!/bin/bash

n_gpus=0
n_procs=1
train_method=NN
generate_arch=1
generate_data=1
compute_kernel=1
compute_bound=1
train=1
sample_funs=0
number_inits=2

export py=/usr/bin/python3
export mpi=/usr/local/shared/openmpi/4.0.0/bin/mpiexec
#py=/users/guillefix/anaconda3/envs/venv/bin/python
#mpi=/users/guillefix/anaconda3/envs/venv/bin/mpiexec
export n_gpus n_procs train_method generate_arch generate_data compute_kernel compute_bound train sample_funs py mpi

dataset=$1
net=$2
#optimizer=sgd
optimizer=adam
#loss=ce
loss=mse
sigmaw=1.41
#sigmaw=50.0
sigmab=0.1
pool=$3
c=0.0
batch_size=32
epochs_after_fit=0
#epochs_after_fit=64
#prefix=32_batch_oddeven_big_sgd_${loss}_sample_
#prefix=oddeven_big_sgd_${loss}_sample_${loss}_
prefix=msweep_runs_

if [ $net = fc ]; then
    L=2
    intermediate_pooling=00
elif [ $net = cnn ]; then
    L=4
    if [ $pool = none ]; then
        intermediate_pooling=0000
    else
        intermediate_pooling=1111
    fi
fi

if [ $loss = ce ]; then
    kern_mult=1
else
    kern_mult=1
fi

for m in 1 3 11 36 122 407 1357 4516 15026 49999; do
  ./run_experiment --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --sigmaw $sigmaw --sigmab $sigmab --n_gpus $n_gpus --pooling $pool -loss $loss --n_samples_repeats 0.1 --kernel_mult $kern_mult --optimizer $optimizer --norandom_training_set --number_inits $number_inits --binarization_method=oddeven --batch_size=$batch_size
done
