#!/bin/bash

test_fun=$1
logPGPEP=$2
errors=$3
n_gpus=0
n_procs=1
train_method=GP
generate_arch=1
generate_data=1
compute_kernel=$4
compute_posterior_params=$4
compute_bound=0
train=0
sample_funs=0
compute_logQ=1
number_inits=1

export py=python3
export mpi=mpiexec
#py=/users/guillefix/anaconda3/envs/venv/bin/python
#mpi=/users/guillefix/anaconda3/envs/venv/bin/mpiexec
export n_gpus n_procs train_method generate_arch generate_data compute_kernel compute_bound train sample_funs compute_logQ compute_posterior_params py mpi

m=100
dataset=mnist
#boolfun=00001110110011111001111111001111000000000000000000000000000000000000000001001100000000001101110100000000000000000000000000000000
#boolfun_comp=84.0
#net=resnet50
net=fc
#net=fc
L=2
#optimizer=sgd
optimizer=adam
#loss=ce
loss=mse
sigmaw=1.41
#sigmaw=50.0
sigmab=0.1
pool=max
c=0.0
batch_size=32
epochs_after_fit=0
#epochs_after_fit=64
#prefix=32_batch_oddeven_big_sgd_${loss}_sample_
#prefix=oddeven_big_sgd_${loss}_sample_${loss}_
prefix=mnist_mse_logQs_

if [ $net = fc ]; then
    L=2
    intermediate_pooling=00
elif [ $net = cnn ]; then
    L=4
    if [ $pool = none ]; then
        intermediate_pooling=0000
    else
        intermediate_pooling=1111
    fi
fi

if [ $loss = ce ]; then
    kern_mult=1
else
    kern_mult=1
fi

#for m in 4 16 32 64 128 256 512 1024 2048 4096 8192; do
#echo $m
  #./run_experiment --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --sigmaw $sigmaw --sigmab $sigmab --n_gpus $n_gpus --pooling $pool -loss $loss --n_samples_repeats 0.1 --kernel_mult $kern_mult --optimizer $optimizer --norandom_training_set --number_inits $number_inits --binarization_method=oddeven --batch_size=$batch_size

  ./run_experiment_logQs --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --sigmaw $sigmaw --sigmab $sigmab --n_gpus $n_gpus --pooling $pool -loss $loss --n_samples_repeats 0.1 --kernel_mult $kern_mult --optimizer $optimizer --number_inits $number_inits --batch_size=$batch_size --layer_widths 512 --batch_size 256 --intermediate_pooling $intermediate_pooling --test_function_size 100  --binarization_method=oddeven --norandom_training_set --test_fun_override $test_fun
  #./run_experiment --prefix $prefix --m $m --dataset $dataset --network $net --number_layers $L --sigmaw $sigmaw --sigmab $sigmab --n_gpus $n_gpus --pooling $pool -loss $loss --epochs_after_fit $epochs_after_fit --n_samples_repeats 0.1 --optimizer $optimizer --number_inits $number_inits --batch_size=$batch_size --intermediate_pooling $intermediate_pooling --kernel_mult 1 --empirical_kernel_batch_size 1024 --normalize_kernel
#done
